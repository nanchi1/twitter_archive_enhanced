{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangled Report"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "            REPORT ON Wrangle 'WeRateDogs' Twitter Data Project\n",
    "            \n",
    "Introduction:\n",
    "\n",
    "The dataset wrangled in this  project is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a twitter account that rates people’s dogs with comments about the dogs.\n",
    "The WeRateDogs Twitter project goals include:\n",
    "\n",
    "In wrangling the dataset provided, I followed the following steps:\n",
    "\n",
    "    ▪ Gathering Data \n",
    "    ▪ Assessing Data \t\n",
    "    ▪ Cleaning Data\n",
    "    • Storing, analyzing and visualizing your wrangled data \n",
    "    • Reporting on the data wrangling efforts and data analyse and visualization.\n",
    "\n",
    "1.\tGathering of Data:\n",
    "    In this section, I gathered 3 dataset using 3 different methods as shown below\n",
    "        \n",
    "    Dataset 1: Twitter archive - I loaded the dataframe from a csv file twitter-archive-enhanced.csv\n",
    "\n",
    "    Dataset 2: image prediction - Programmatically I made a http request to https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv and downloaded the file into image-prediction.tsv. I loaded the .tsv into a dataframe with tab(\\t) as the column separator.\n",
    "\n",
    "   \tDataset 3: Additional data (tweet's retweet count and favorite (\"like\") count) From Twitter API: \n",
    "    ▪ I set up consumer secret and token in a variable as well as access token and secret\n",
    "    ▪ I setup a oauth object using the variables created above\n",
    "    ▪ With the tweet archive above, I looped through the tweets and made a call to twitter api for each tweet_id to retrieve tweet status such as retweet_count\tfavorite_count.\n",
    "    ▪ I dumped the result in a file tweet_json.json\n",
    "    ▪ Using json.loads, I moved each json record into an array. I then loaded the array into a dataframe\n",
    "    \n",
    "The purpose of gathering is to Import the  data into our programming environment (Jupyter Notebook).\n",
    "\n",
    "2.\tAssessing Data:\n",
    " Once the data was gathered, I began to assess the data on both quality and tidiness issues.\n",
    "\n",
    "    Quality Issues Identified:\n",
    "    1.\ttwitter_archive_enhanced-2.csv:\n",
    "        •\tSome supposed Date and Time fields are marked defined as Object datatypes. I will convert such fields to timestamp\n",
    "        •\tTweet_id is an int64 type\n",
    "        •\tPresence of values such as my, an, None, quite, space, such, the, this in name doggo floofer pupper puppo columns\n",
    "        •\tRetweets are present and need to be deleted\n",
    "        •\tdoggo, floofer, pupper,puppo columns can be merged into one\n",
    "\n",
    "    2.\timage_predictions.tsv\n",
    "        •\tSome duplicate values exit in jpg_url\n",
    "        •\ttweet_id is an int64 type\n",
    "    3.\ttweet_json\n",
    "        •\tid which is presumed to be tweet_id is an int64 type. \n",
    "\n",
    "    Tidiness Issues Identified:\n",
    "    1.\tConvert date fields to timestamp\n",
    "    2.\tJoin the 3 dataset with tweet_id as join key\n",
    "    3.\tUse a single column to show dog growth stage instead of 4 columns\n",
    "    4.   Remove the invalid numerators\n",
    "    5.   The source names are embedded in html anchor tags. I will extract the sources from within the tags\n",
    "    \n",
    "3.  Cleaning Data:\n",
    "After the assessment, I cleaned the data through some of the basic python functions which I used at the Define, Code and Test as shown below to:\n",
    "         • Merge the clean versions of archive, images, and twitter_counts_df data frames Correct the dog types.\n",
    "         • Create one column for the various dog types: doggo, floofer, pupper, puppo Remove columns no longer needed: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp.\n",
    "         • Some supposed Date and Time fields are marked defined as Object datatypes. I will convert such fields to timestam\n",
    "         • tweet_id field is numeric. I will convert to string to make inner join easier in the later part of this exercise.\n",
    "            I will also rename id to tweet_id on tweet_json dataframe.\n",
    "            Rename 'id' column to 'tweet_id' and Convert tweet_id to string datatype\n",
    "         • In image_predictions,duplicates exist in jpg_url. I will delete the duplicates.\n",
    "         • Invalid names exist in the name column, I will remove those. \n",
    "         • The denominator for ratings are different. I will express this as a ratio to make comparison easier. \n",
    "         • 4 columns are used to specify dog growth stage. I will combine this into a single column to make analysis easier.\n",
    "         \n",
    "After cleaning the three dataframes, I made copies of them to meet any eventuality so as not to affect the original copy .\n",
    "        twitter_archive_copy_enhanced_dataframe=twitter_archive_enhanced_dataframe.copy()\n",
    "        image_predictions_copy_dataframe=image_predictions_dataframe.copy()\n",
    "        tweet_json_copy_dataframe=tweet_json_dataframe.copy()\n",
    "        tweet_json_trimmed_copy_dataframe=tweet_json_trimmed_dataframe.copy()\n",
    "\n",
    "4.\tConclusion and Storing of Data\n",
    "After analyzing and cleaning the dataset, I saved the merged dataframe into twitter_archive_master.csv file\n",
    "        merged_dataframes.to_csv('twitter_archive_master.csv', encoding = 'utf-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
